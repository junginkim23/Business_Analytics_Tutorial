{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QFHP0Js1JBSs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama in c:\\users\\junginkim\\anaconda3\\envs\\class\\lib\\site-packages (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4gCQgfpOHkb7"
   },
   "outputs": [],
   "source": [
    "import os, math, sys, argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7-gtzL_IHmEn"
   },
   "outputs": [],
   "source": [
    "# Custom Transform 함수 정의 --> 2가지 종류의 Augmentation 산출\n",
    "\n",
    "class Transform_Twice:\n",
    "    \n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        out1 = self.transform(img)\n",
    "        out2 = self.transform(img)\n",
    "        \n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "3sOUFnwmHmHc"
   },
   "outputs": [],
   "source": [
    "# Labeled data를 생성하는 함수\n",
    "\n",
    "class Labeled_CIFAR10(torchvision.datasets.CIFAR10):\n",
    "    \n",
    "    def __init__(self, root, indices=None,\n",
    "                train=True, transform=None,\n",
    "                target_transform=None, download=False):\n",
    "        \n",
    "        super(Labeled_CIFAR10, self).__init__(root,\n",
    "                                        train=train,\n",
    "                                        transform=transform,\n",
    "                                        target_transform=target_transform,\n",
    "                                        download=download)\n",
    "\n",
    "        if indices is not None:\n",
    "            self.data = self.data[indices]\n",
    "            self.targets = np.array(self.targets)[indices]\n",
    "        \n",
    "        self.data = Transpose(Normalize(self.data))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "mI3l2iwGHmJy"
   },
   "outputs": [],
   "source": [
    "# Unlabeled data를 생성하는 함수\n",
    "\n",
    "'''\n",
    "Unlabeled data의 Label은 -1로 지정\n",
    "'''\n",
    "\n",
    "class Unlabeled_CIFAR10(Labeled_CIFAR10):\n",
    "    \n",
    "    def __init__(self, root, indices, train=True, transform=None, target_transform=None, download=False):\n",
    "        \n",
    "        super(Unlabeled_CIFAR10, self).__init__(root, indices, train,\n",
    "                                            transform=transform,\n",
    "                                            target_transform=target_transform,\n",
    "                                            download=download)\n",
    "        \n",
    "        self.targets = np.array([-1 for i in range(len(self.targets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PNJw-5pdHmMe"
   },
   "outputs": [],
   "source": [
    "# 데이터셋을 분할하기 위해서 Index를 섞는 함수 정의\n",
    "\n",
    "def split_datasets(labels, n_labeled_per_class):\n",
    "    \n",
    "    '''\n",
    "    - n_labeled_per_class: labeled data의 개수\n",
    "    - 클래스 내 500개 데이터는 validation data로 정의\n",
    "    - 클래스 당 n_labeled_per_class 개수 만큼 labeled data로 정의\n",
    "    - 나머지 이미지는 unlabeled data로 정의\n",
    "    '''\n",
    "    \n",
    "    ### labeled, unlabeled, validation data 분할할 list 초기화\n",
    "    labels = np.array(labels, dtype=int) \n",
    "    indice_labeled, indice_unlabeled, indice_val = [], [], [] \n",
    "    \n",
    "    ### 각 class 단위로 loop 생성\n",
    "    for i in range(10): \n",
    "\n",
    "        # 각각 labeled, unlabeled, validation data를 할당\n",
    "        indice_tmp = np.where(labels==i)[0]\n",
    "        \n",
    "        indice_labeled.extend(indice_tmp[: n_labeled_per_class])\n",
    "        indice_unlabeled.extend(indice_tmp[n_labeled_per_class: -500])\n",
    "        indice_val.extend(indice_tmp[-500: ])\n",
    "    \n",
    "    ### 각 index를 Shuffle\n",
    "    for i in [indice_labeled, indice_unlabeled, indice_val]:\n",
    "        np.random.shuffle(i)\n",
    "    \n",
    "    return indice_labeled, indice_unlabeled, indice_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "45-rzXQKHmPE"
   },
   "outputs": [],
   "source": [
    "# CIFAR10에 대하여 labeled, unlabeled, validation, test dataset 생성\n",
    "\n",
    "def get_cifar10(data_dir: str, n_labeled: int,\n",
    "                transform_train=None, transform_val=None,\n",
    "                download=True):\n",
    "    \n",
    "    ### Torchvision에서 제공해주는 CIFAR10 dataset Download\n",
    "    base_dataset = torchvision.datasets.CIFAR10(data_dir, train=True, download=download)\n",
    "    \n",
    "    ### labeled, unlabeled, validation data에 해당하는 index를 가져오기\n",
    "    indice_labeled, indice_unlabeled, indice_val = split_datasets(base_dataset.targets, int(n_labeled/10)) ### n_labeled는 아래 MixMatch_argparser 함수에서 정의\n",
    "    \n",
    "    ### index를 기반으로 dataset을 생성\n",
    "    '''\n",
    "    왜 unlabeled가 Transform_twice가 적용되었을까?\n",
    "    '''\n",
    "    train_labeled_set = Labeled_CIFAR10(data_dir, indice_labeled, train=True, transform=transform_train) \n",
    "    train_unlabeled_set = Unlabeled_CIFAR10(data_dir, indice_unlabeled, train=True, transform=Transform_Twice(transform_train))\n",
    "    val_set = Labeled_CIFAR10(data_dir, indice_val, train=True, transform=transform_val, download=True) \n",
    "    test_set = Labeled_CIFAR10(data_dir, train=False, transform=transform_val, download=True) \n",
    "\n",
    "    return train_labeled_set, train_unlabeled_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4Wc-3Z1fHmRi"
   },
   "outputs": [],
   "source": [
    "# Image를 전처리 하기 위한 함수\n",
    "\n",
    "### 데이터를 정규화 하기 위한 함수\n",
    "def Normalize(x, m=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2345, 0.2616)):\n",
    "        \n",
    "    ##### x, m, std를 각각 array화\n",
    "    x, m, std = [np.array(a, np.float32) for a in (x, m, std)] \n",
    "\n",
    "    ##### 데이터 정규화\n",
    "    x -= m * 255 \n",
    "    x *= 1.0/(255*std)\n",
    "    return x\n",
    "\n",
    "### 데이터를 (B, C, H, W)로 수정해주기 위한 함수 (from torchvision.transforms 내 ToTensor 와 동일한 함수)\n",
    "def Transpose(x, source='NHWC', target='NCHW'):\n",
    "    return x.transpose([source.index(d) for d in target])\n",
    "\n",
    "### 특정 이미지에 동서남북 방향으로 4만큼 픽셀을 추가해주기 위한 학습\n",
    "def pad(x, border=4):\n",
    "    return np.pad(x, [(0, 0), (border, border), (border, border)], mode='reflect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "lYm7Pt1vHmUL"
   },
   "outputs": [],
   "source": [
    "# Image를 Augmentation하기 위한 함수\n",
    "\n",
    "### Image를 Padding 및 Crop적용\n",
    "'''\n",
    "1. object는 써도 되고 안써도 되는 것\n",
    "2. assert는 오류를 유도하기 위함 (나중에 이렇게 해놓으면 디버깅이 편함) --> 여기선 적절한 데이터 인풋의 형태를 유도\n",
    "'''\n",
    "class RandomPadandCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = pad(x, 4)\n",
    "        \n",
    "        old_h, old_w = x.shape[1: ]\n",
    "        new_h, new_w = self.output_size\n",
    "        \n",
    "        top = np.random.randint(0, old_h-new_h)\n",
    "        left = np.random.randint(0, old_w-new_w)\n",
    "        \n",
    "        x = x[:, top:top+new_h, left:left+new_w]\n",
    "        return x\n",
    "    \n",
    "    \n",
    "### RandomFlip하는 함수 정의\n",
    "class RandomFlip(object):\n",
    "    def __call__(self, x):\n",
    "        if np.random.rand() < 0.5:\n",
    "            x = x[:, :, ::-1]\n",
    "        \n",
    "        return x.copy()\n",
    "    \n",
    "    \n",
    "### GaussianNoise를 추가하는 함수 정의\n",
    "class GaussianNoise(object):\n",
    "    def __call__(self, x):\n",
    "        c, h, w = x.shape\n",
    "        x += np.random.randn(c, h, w)*0.15\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "SJcg4AIpHmbo"
   },
   "outputs": [],
   "source": [
    "# Numpy를 Tensor로 변환하는 함수\n",
    "class ToTensor(object):\n",
    "    def __call__(self, x):\n",
    "        x = torch.from_numpy(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WideResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_a9lgAOhHmeF"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0, activate_before_residual=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "        self.activate_before_residual = activate_before_residual\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut and self.activate_before_residual == True:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0F-BjNbBH8z1"
   },
   "outputs": [],
   "source": [
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0, activate_before_residual=False):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate, activate_before_residual))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "iP1ftgvYH82i"
   },
   "outputs": [],
   "source": [
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, num_classes, depth=28, widen_factor=2, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate, activate_before_residual=True)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3], momentum=0.001)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised loss function\n",
    "#### Semi-supervised loss = Loss(Labeled, x) + lambda * Loss(Unlabeled, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "1HxAJDO2IoFA"
   },
   "outputs": [],
   "source": [
    "class Loss_Semisupervised(object):\n",
    "    def __call__(self, args, outputs_x, target_x, outputs_u, targets_u, epoch):\n",
    "        self.args = args\n",
    "        probs_u = torch.softmax(outputs_u, dim=1)\n",
    "\n",
    "        loss_x = -torch.mean(\n",
    "            torch.sum(F.log_softmax(outputs_x, dim=1)*target_x, dim=1)\n",
    "        )\n",
    "\n",
    "        loss_u = torch.mean((probs_u-targets_u)**2)\n",
    "\n",
    "        return loss_x, loss_u, self.args.lambda_u*linear_rampup(epoch, self.args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "CfMVPGCvIoOt"
   },
   "outputs": [],
   "source": [
    "def linear_rampup(current, rampup_length):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current/rampup_length, 0.0, 1.0)\n",
    "        return float(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "3KyEyMQPIobG"
   },
   "outputs": [],
   "source": [
    "class WeightEMA(object): # EMA=Exponential Moving Average\n",
    "    \n",
    "    '''\n",
    "    이를 하는 이유는 학습시간이 길어지거나, Trivial Solution을 방지, 과적합 방지 등. --> 가중치를 업데이트 시 a(최근가중치)+(1-a)(이전가중치)\n",
    "    '''\n",
    "    def __init__(self, model, ema_model, lr, alpha=0.999):\n",
    "        self.model = model\n",
    "        self.ema_model = ema_model\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.params = list(self.model.state_dict().items())\n",
    "        self.ema_params = list(self.ema_model.state_dict().items())\n",
    "\n",
    "        self.wd = 0.02 * lr\n",
    "\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            param[1].data.copy_(ema_param[1].data)\n",
    "    \n",
    "    def step(self):\n",
    "        inverse_alpha = 1.0 - self.alpha\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            if ema_param[1].dtype == torch.float32:\n",
    "                ema_param[1].mul_(self.alpha) # ema_params_new = self.alpha * ema_params_old\n",
    "                ema_param[1].add_(param[1]*inverse_alpha) # ema_params_Double_new = (1-self.alpha)*params\n",
    "\n",
    "                # summary: ema_params_new = self.alpha*ema_params_old + (1-self.alpha)*params\n",
    "                # params: 학습되고 있는 모델 parameter\n",
    "                param[1].mul_(1-self.wd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "qoESnZ41IzOZ"
   },
   "outputs": [],
   "source": [
    "def interleave_offsets(batch_size, nu):\n",
    "    \n",
    "    '''\n",
    "    이것도 assert의 목적으로 활용되는 code\n",
    "    '''\n",
    "    \n",
    "    groups = [batch_size//(nu+1)]*(nu+1)\n",
    "    for x in range(batch_size-sum(groups)):\n",
    "        groups[-x-1] += 1\n",
    "\n",
    "    offsets = [0]\n",
    "    for g in groups:\n",
    "        offsets.append(offsets[-1]+g)\n",
    "    \n",
    "    assert offsets[-1] == batch_size\n",
    "    return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "pW0CqEc7IokG"
   },
   "outputs": [],
   "source": [
    "def interleave(xy, batch_size):\n",
    "    \n",
    "    '''\n",
    "    이것도 assert의 목적으로 활용되는 code\n",
    "    '''\n",
    "    \n",
    "    nu = len(xy) - 1\n",
    "    offsets = interleave_offsets(batch_size, nu)\n",
    "\n",
    "    xy = [[v[offsets[p]:offsets[p+1]] for p in range(nu+1)] for v in xy]\n",
    "    for i in range(1, nu+1):\n",
    "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
    "    return [torch.cat(v, dim=0) for v in xy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "0ekZyGFLI7MS"
   },
   "outputs": [],
   "source": [
    "def get_tqdm_config(total, leave=True, color='white'):\n",
    "    fore_colors = {\n",
    "        'red': Fore.LIGHTRED_EX,\n",
    "        'green': Fore.LIGHTGREEN_EX,\n",
    "        'yellow': Fore.LIGHTYELLOW_EX,\n",
    "        'blue': Fore.LIGHTBLUE_EX,\n",
    "        'magenta': Fore.LIGHTMAGENTA_EX,\n",
    "        'cyan': Fore.LIGHTCYAN_EX,\n",
    "        'white': Fore.LIGHTWHITE_EX,\n",
    "    }\n",
    "    return {\n",
    "        'file': sys.stdout,\n",
    "        'total': total,\n",
    "        'desc': \" \",\n",
    "        'dynamic_ncols': True,\n",
    "        'bar_format':\n",
    "            \"{l_bar}%s{bar}%s| [{elapsed}<{remaining}, {rate_fmt}{postfix}]\" % (fore_colors[color], Fore.RESET),\n",
    "        'leave': leave\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric\n",
    "#### top1 accuracy, top5 accuracy\n",
    "#### top1 accuracy: (확률 값이 가장 높은 범주와 실제 범주가 일치하는 관측치 수)/ 전체 관측치\n",
    "#### top5 accuracy: (확률 값 상위 5개 중 실제 범주가 존재하는 관측치 수)/ 전체 관측치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "6sDCabk8JaGy"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1, )):\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        if k == 1:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        if k > 1:\n",
    "            correct_k = correct[:k].float().sum(0).sum(0)\n",
    "        acc = correct_k.mul_(100.0 / batch_size)\n",
    "        acc = acc.detach().cpu().numpy()\n",
    "        res.append(acc)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "njMqQgRKH85I"
   },
   "outputs": [],
   "source": [
    "class MixMatchTrainer():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        root_dir = '/content/MixMatch' # PROJECT directory\n",
    "        self.experiment_dir = os.path.join(root_dir, 'results') # 학습된 모델을 저장할 폴더 경로 정의 및 폴더 생성\n",
    "        os.makedirs(self.experiment_dir, exist_ok=True)\n",
    "\n",
    "        name_exp = \"_\".join([str(self.args.n_labeled), str(self.args.T)]) # 주요 하이퍼 파라미터로 폴더 저장 경로 지정 \n",
    "        self.experiment_dir = os.path.join(self.experiment_dir, name_exp)\n",
    "        os.makedirs(self.experiment_dir, exist_ok=True)\n",
    "\n",
    "        # Data\n",
    "        print(\"==> Preparing CIFAR10 dataset\")\n",
    "        transform_train = transforms.Compose([\n",
    "            RandomPadandCrop(32),\n",
    "            RandomFlip(),\n",
    "            ToTensor()\n",
    "        ]) # 학습에 사용할 data augmentation 정의\n",
    "\n",
    "        transform_val = transforms.Compose([\n",
    "            ToTensor()\n",
    "        ]) # validation, test dataset에 대한 data augmentation 정의\n",
    "           # 합성곱 신경망에 입력 될 수 있도록만 지정(Augmentation 사용하지 않는 것과 동일)\n",
    "\n",
    "        train_labeled_set, train_unlabeled_set, val_set, test_set = \\\n",
    "            get_cifar10(\n",
    "                data_dir=os.path.join(root_dir, 'data'),\n",
    "                n_labeled=self.args.n_labeled,\n",
    "                transform_train=transform_train,\n",
    "                transform_val=transform_val\n",
    "            ) # 앞에서 정의한 (def) get_cifar10 함수에서 train_labeled, train_unlabeled, validation, test dataset\n",
    "        \n",
    "        # DataLoader 정의\n",
    "        self.labeled_loader = DataLoader(\n",
    "            dataset=train_labeled_set,\n",
    "            batch_size=self.args.batch_size,\n",
    "            shuffle=True, num_workers=0, drop_last=True\n",
    "        )\n",
    "\n",
    "        self.unlabeled_loader = DataLoader(\n",
    "            dataset=train_unlabeled_set,\n",
    "            batch_size=self.args.batch_size,\n",
    "            shuffle=True, num_workers=0, drop_last=True\n",
    "        )\n",
    "\n",
    "        self.val_loader = DataLoader(\n",
    "            dataset=val_set, shuffle=False, num_workers=0, drop_last=False\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            dataset=test_set, shuffle=False, num_workers=0, drop_last=False\n",
    "        )\n",
    "\n",
    "        # Build WideResNet\n",
    "        print(\"==> Preparing WideResNet\")\n",
    "        self.model = self.create_model(ema=False)\n",
    "        self.ema_model = self.create_model(ema=True)\n",
    "\n",
    "        # Define loss functions\n",
    "        self.criterion_train = Loss_Semisupervised()\n",
    "        self.criterion_val = nn.CrossEntropyLoss().to(self.args.cuda)\n",
    "\n",
    "        # Define optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.ema_optimizer = WeightEMA(self.model, self.ema_model, lr=self.args.lr, alpha=self.args.ema_decay)\n",
    "\n",
    "        # 학습 결과를 저장할 Tensorboard 정의\n",
    "        self.writer = SummaryWriter(self.experiment_dir)\n",
    "\n",
    "    def create_model(self, ema=False):\n",
    "        # Build WideResNet & EMA model\n",
    "        model = WideResNet(num_classes=10)\n",
    "        model = model.to(self.args.cuda)\n",
    "\n",
    "        if ema:\n",
    "            for param in model.parameters():\n",
    "                param.detach_()\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        # 모델 학습 함수\n",
    "        losses_t, losses_x, losses_u, ws = 0.0, 0.0, 0.0, 0.0\n",
    "        self.model.train()\n",
    "\n",
    "        # iter & next remind\n",
    "        # iter: list 내 batch size 만큼 랜덤하게 불러오게 하는 함수\n",
    "        # next: iter 함수가 작동하도록 하는 명령어\n",
    "        iter_labeled = iter(self.labeled_loader)\n",
    "        iter_unlabeled = iter(self.unlabeled_loader)\n",
    "\n",
    "        with tqdm(**get_tqdm_config(total=self.args.num_iter,\n",
    "                leave=True, color='blue')) as pbar:\n",
    "            for batch_idx in range(self.args.num_iter):\n",
    "                # 왜 try-except 문을 사용하나?\n",
    "                # 코드 작성 후 iter&next가 정확히 작용하지 않는 경우가 있음을 확인\n",
    "                # 다시 iter_labeled, iter_unlabeled를 정의해 학습에 문제가 없도록 다시 선언\n",
    "                try:\n",
    "                    inputs_x, targets_x = next(iter_labeled)\n",
    "                except:\n",
    "                    iter_labeled = iter(self.labeled_loader)\n",
    "                    inputs_x, targets_x = next(iter_labeled)\n",
    "                real_B = inputs_x.size(0)\n",
    "\n",
    "                # Transform label to one-hot\n",
    "                targets_x = torch.zeros(real_B, 10).scatter_(1, targets_x.view(-1,1).long(), 1)\n",
    "                inputs_x, targets_x = inputs_x.to(self.args.cuda), targets_x.to(self.args.cuda)\n",
    "\n",
    "                try:\n",
    "                    tmp_inputs, _ = next(iter_unlabeled)\n",
    "                except:\n",
    "                    iter_unlabeled = iter(self.unlabeled_loader)\n",
    "                    tmp_inputs, _ = next(iter_unlabeled)\n",
    "\n",
    "                inputs_u1, inputs_u2 = tmp_inputs[0], tmp_inputs[1]\n",
    "                inputs_u1, inputs_u2 = inputs_u1.to(self.args.cuda), inputs_u2.to(self.args.cuda)\n",
    "\n",
    "                # Unlabeled data에 대한 실제 값 생성\n",
    "                # 서로 다른 Augmentation 결과의 출력 값의 평균 계산\n",
    "                # Temperature 값으로 실제 값 스케일링\n",
    "                with torch.no_grad():\n",
    "                    outputs_u1 = self.model(inputs_u1)\n",
    "                    outputs_u2 = self.model(inputs_u2)\n",
    "\n",
    "                    pt = (torch.softmax(outputs_u1, dim=1)+torch.softmax(outputs_u2, dim=1)) / 2\n",
    "                    pt = pt**(1/self.args.T)\n",
    "\n",
    "                    targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
    "                    targets_u = targets_u.detach()\n",
    "                \n",
    "                # MixUp\n",
    "                # 서로 다른 이미지와 레이블을 섞는 작업\n",
    "                # feature space 상에서 범주 별 Decision boundary를 정확하게 잡아주는 역할\n",
    "                inputs = torch.cat([inputs_x, inputs_u1, inputs_u2], dim=0)\n",
    "                targets = torch.cat([targets_x, targets_u, targets_u], dim=0)\n",
    "\n",
    "                l_mixup = np.random.beta(self.args.alpha, self.args.alpha)\n",
    "                l_mixup = max(l_mixup, 1-l_mixup)\n",
    "\n",
    "                # inputs의 index를 섞어 서로 다른 범주끼리 섞도록 하는 역할\n",
    "                B = inputs.size(0)\n",
    "                random_idx = torch.randperm(B)\n",
    "\n",
    "                inputs_a, inputs_b = inputs, inputs[random_idx]\n",
    "                targets_a, targets_b = targets, targets[random_idx]\n",
    "\n",
    "                mixed_input = l_mixup*inputs_a + (1-l_mixup)*inputs_b\n",
    "                mixed_target = l_mixup*targets_a + (1-l_mixup)*targets_b\n",
    "\n",
    "                # batch size 만큼 분할 진행 (2N, C, H, W) -> (N, C, H, W) & (N, C, H, W)\n",
    "                # 앞 부분은 labeled, 뒷 부분은 unlabeled\n",
    "                '''\n",
    "                이렇게 하는 이유는 첫 B는 Label 데이터로 활용, 나중 B는 Unlabeled data로 활용하기 위함 (관용적 활용법)\n",
    "                '''\n",
    "                \n",
    "                mixed_input = list(torch.split(mixed_input, real_B))\n",
    "                mixed_input = interleave(mixed_input, real_B)\n",
    "\n",
    "                logits = [self.model(mixed_input[0])] # for labeled\n",
    "                for input in mixed_input[1:]:\n",
    "                    logits.append(self.model(input)) # for unlabeled\n",
    "\n",
    "                logits = interleave(logits, real_B) # interleave: 정확히 섞이었는지 확인\n",
    "                logits_x = logits[0]\n",
    "                logits_u = torch.cat(logits[1:], dim=0)\n",
    "\n",
    "                loss_x, loss_u, w = \\\n",
    "                    self.criterion_train(self.args,\n",
    "                                    logits_x, mixed_target[:real_B],\n",
    "                                    logits_u, mixed_target[real_B:],\n",
    "                                    epoch+batch_idx/self.args.num_iter) # Semi-supervised loss 계산\n",
    "\n",
    "                loss = loss_x + w * loss_u\n",
    "\n",
    "                # Backpropagation and Model parameter update\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.ema_optimizer.step()\n",
    "\n",
    "                losses_x += loss_x.item()\n",
    "                losses_u += loss_u.item()\n",
    "                losses_t += loss.item()\n",
    "                ws += w\n",
    "\n",
    "                self.writer.add_scalars(\n",
    "                    'Training steps', {\n",
    "                        'Total_loss': losses_t/(batch_idx+1),\n",
    "                        'Labeled_loss':losses_x/(batch_idx+1),\n",
    "                        'Unlabeled_loss':losses_u/(batch_idx+1),\n",
    "                        'W values': ws/(batch_idx+1)\n",
    "                    }, global_step=epoch*self.args.batch_size+batch_idx\n",
    "                )\n",
    "\n",
    "                pbar.set_description(\n",
    "                    '[Train(%4d/ %4d)-Total: %.3f|Labeled: %.3f|Unlabeled: %.3f]'%(\n",
    "                        (batch_idx+1), self.args.num_iter,\n",
    "                        losses_t/(batch_idx+1), losses_x/(batch_idx+1), losses_u/(batch_idx+1)\n",
    "                    )\n",
    "                )\n",
    "                pbar.update(1)\n",
    "\n",
    "            pbar.set_description(\n",
    "                '[Train(%4d/ %4d)-Total: %.3f|Labeled: %.3f|Unlabeled: %.3f]'%(\n",
    "                    epoch, self.args.epochs,\n",
    "                    losses_t/(batch_idx+1), losses_x/(batch_idx+1), losses_u/(batch_idx+1)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return losses_t/(batch_idx+1), losses_x/(batch_idx+1), losses_u/(batch_idx+1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, epoch, phase):\n",
    "        self.ema_model.eval()\n",
    "\n",
    "        # Train, Validation, Test dataset 에 대한 DataLoader를 정의\n",
    "        if phase == 'Train':\n",
    "            data_loader = self.labeled_loader\n",
    "            c = 'blue'\n",
    "        elif phase == 'Valid':\n",
    "            data_loader = self.val_loader\n",
    "            c = 'green'\n",
    "        elif phase == 'Test ':        \n",
    "            data_loader = self.test_loader\n",
    "            c = 'red'\n",
    "\n",
    "        losses = 0.0\n",
    "        top1s, top5s = [], []\n",
    "\n",
    "        with tqdm(**get_tqdm_config(total=len(data_loader),\n",
    "                leave=True, color=c)) as pbar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "                \n",
    "                targets = targets.type(torch.LongTensor)\n",
    "                inputs, targets = inputs.to(self.args.cuda), targets.to(self.args.cuda)\n",
    "                outputs = self.ema_model(inputs)\n",
    "                \n",
    "                loss = self.criterion_val(outputs, targets)\n",
    "                # labeled dataset에 대해서만 손실함수 계산\n",
    "                # torch.nn.CrossEntropyLoss()를 사용해서 손실함수 계산\n",
    "\n",
    "                prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "                losses += loss.item()\n",
    "                top1s.append(prec1)\n",
    "                top5s.append(prec5)\n",
    "\n",
    "                self.writer.add_scalars(\n",
    "                    f'{phase} steps', {\n",
    "                        'Total_loss': losses/(batch_idx+1),\n",
    "                        'Top1 Acc': np.mean(top1s),\n",
    "                        'Top5 Acc': np.mean(top5s)\n",
    "                    }, global_step=epoch*self.args.batch_size+batch_idx\n",
    "                )\n",
    "\n",
    "                pbar.set_description(\n",
    "                    '[%s-Loss: %.3f|Top1 Acc: %.3f|Top5 Acc: %.3f]'%(\n",
    "                        phase,\n",
    "                        losses/(batch_idx+1), np.mean(top1s), np.mean(top5s)\n",
    "                    )\n",
    "                )\n",
    "                pbar.update(1)\n",
    "\n",
    "            pbar.set_description(\n",
    "                '[%s(%4d/ %4d)-Loss: %.3f|Top1 Acc: %.3f|Top5 Acc: %.3f]'%(\n",
    "                    phase,\n",
    "                    epoch, self.args.epochs,\n",
    "                    losses/(batch_idx+1), np.mean(top1s), np.mean(top5s)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return losses/(batch_idx+1), np.mean(top1s), np.mean(top5s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparamters\n",
    "#### argparser라는 패키지를 이용해 각종 hyperparameter 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "wscpGd8NH87k"
   },
   "outputs": [],
   "source": [
    "def MixMatch_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"MixMatch PyTorch Implementation for BA\")\n",
    "    \n",
    "    # method arguments\n",
    "    parser.add_argument('--n-labeled', type=int, default=4000)\n",
    "    parser.add_argument('--num-iter', type=int, default=100,\n",
    "                        help=\"The number of iteration per epoch\")\n",
    "    parser.add_argument('--alpha', type=float, default=0.75)\n",
    "    parser.add_argument('--lambda-u', type=float, default=75)\n",
    "    parser.add_argument('--T', default=0.5, type=float)\n",
    "    parser.add_argument('--ema-decay', type=float, default=0.999)\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=30)\n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument('--lr', type=float, default=0.02)\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ZEjxA_eSH896"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = MixMatch_parser()\n",
    "    args = parser.parse_args([])\n",
    "    args.cuda = torch.device(\"cuda:0\")\n",
    "\n",
    "    trainer = MixMatchTrainer(args)\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    # best_loss of validation 기준으로 모멜 저장\n",
    "\n",
    "    losses, losses_x, losses_u = [], [], []\n",
    "    \n",
    "    train_losses, train_top1s, train_top5s = [], [], []\n",
    "    val_losses, val_top1s, val_top5s = [], [], []\n",
    "    test_losses, test_top1s, test_top5s = [], [], []\n",
    "    # accuracy 증가 속도, loss values 감소 속도를 그래프로 그리기\n",
    "    # list에 각종 값들을 저장\n",
    "    for epoch in range(1, args.epochs+1, 1):\n",
    "        loss, loss_x, loss_u = trainer.train(epoch)\n",
    "        losses.append(loss)\n",
    "        losses_x.append(loss_x)\n",
    "        losses_u.append(loss_u)\n",
    "\n",
    "        loss, top1, top5 = trainer.validate(epoch, 'Train')\n",
    "        train_losses.append(loss)\n",
    "        train_top1s.append(top1)\n",
    "        train_top5s.append(top5)\n",
    "\n",
    "        loss, top1, top5 = trainer.validate(epoch, 'Valid')\n",
    "        val_losses.append(loss)\n",
    "        val_top1s.append(top1)\n",
    "        val_top5s.append(top5)\n",
    "\n",
    "        # validation loss 기준 모델 저장\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            torch.save(trainer.model, os.path.join(trainer.experiment_dir, 'model.pth'))\n",
    "            torch.save(trainer.ema_model, os.path.join(trainer.experiment_dir, 'ema_model.pth'))\n",
    "\n",
    "        loss, top1, top5 = trainer.validate(epoch, 'Test ')\n",
    "        test_losses.append(loss)\n",
    "        test_top1s.append(top1)\n",
    "        test_top5s.append(top5)\n",
    "\n",
    "        torch.save(trainer.model, os.path.join(trainer.experiment_dir, 'checkpooint_model.pth'))\n",
    "        torch.save(trainer.ema_model, os.path.join(trainer.experiment_dir, 'checkpoint_ema_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "eIPcmsY5Hmgz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing CIFAR10 dataset\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Preparing WideResNet\n",
      "[Train(   1/   30)-Total: 2.201|Labeled: 2.185|Unlabeled: 0.004]: 100%|\u001b[94m██████████\u001b[39m| [00:09<00:00, 10.46it/s]\n",
      "[Train(   1/   30)-Loss: 2.298|Top1 Acc: 13.105|Top5 Acc: 54.133]: 100%|\u001b[94m██████████\u001b[39m| [00:00<00:00, 69.89it/s]\n",
      "[Valid-Loss: 2.295|Top1 Acc: 15.904|Top5 Acc: 55.904]:   8%|\u001b[92m▊         \u001b[39m| [00:03<00:36, 125.77it/s]"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxnUlhDyQQW8m0x4Udibre",
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1519005239f2de3440a81beb718df9ab72fdd1ec6a07fd4a7f663a9215b4022"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
